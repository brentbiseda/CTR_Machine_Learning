{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final Project - Clickthrough Rate Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, OneHotEncoderModel\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, Imputer, VectorAssembler, SQLTransformer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "# from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier as RF\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf = pyspark.SparkConf().setAll([ ('spark.executor.memory', '10g'), ('spark.driver.memory','10g')])\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "# sqlContext = SQLContext(sc)\n",
    "# \n",
    "# sc = SparkContext(appName=\"Final_Project\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "app_name = \"final_project_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://docker.w261:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f85b7f22320>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip below and move to next section once imputed file is written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = sqlContext.read.parquet('test.parquet') # This loads a Data Frame\n",
    "trainDF = sqlContext.read.parquet('train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = trainDF.columns\n",
    "INTEGER_FEATURES = HEADER[0:14] # These are the integer features\n",
    "CATEGORICAL_FEATURES = HEADER[14:] # These are the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in INTEGER_FEATURES[1:]:\n",
    "    trainDF = trainDF.withColumn(feature, trainDF[feature].cast(StringType()))\n",
    "    trainDF = trainDF.fillna({feature:''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES = INTEGER_FEATURES + CATEGORICAL_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will track which features to eliminate from our dataframe\n",
    "featuresToDrop = set()\n",
    "featuresToDrop.add('x11')\n",
    "featuresToDrop.add('x13')\n",
    "featuresToDrop.add('x14')\n",
    "featuresToDrop.add('x18')\n",
    "featuresToDrop.add('x21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keepTopK(df, dftest, K, categoricalColumnstoImpute):\n",
    "    for col in categoricalColumnstoImpute:\n",
    "        mostCommon = df.select(col).groupby(col).count()\\\n",
    "                            .orderBy('count', ascending=False) \\\n",
    "                            .limit(K).collect()\n",
    "            \n",
    "        mostCommonSet = set([x[0] for x in mostCommon])\n",
    "               \n",
    "        df = df.withColumn(col, F.when(~df[col].isin(mostCommonSet), \"RECODED\").otherwise(df[col]))\n",
    "        \n",
    "        dftest = dftest.withColumn(col, F.when(~dftest[col].isin(mostCommonSet), \"RECODED\") \\\n",
    "                        .otherwise(dftest[col]))\n",
    "    \n",
    "    print(\"Successfully Recoded Top K Categorical Values\")\n",
    "    \n",
    "    return (df, dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "trainDF, testDF = keepTopK(trainDF, testDF, 10000, ['x5', 'x16', 'x17', 'x25', 'x29', 'x34', 'x37']) # Select 10,000 top categories\n",
    "print(f\"\\n... Executed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Number of Unique Values\n",
    "start = time.time()\n",
    "distinct = []\n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    distinct.append(set(trainDF.select(col).distinct().rdd.map(lambda x: x[0]).collect()))\n",
    "print(f\"\\n... Executed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item, number in zip(CATEGORICAL_FEATURES, distinct):\n",
    "    print(f\"Feature: {item} had {len(number)} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctDict = dict((k, v) for k, v  in zip(CATEGORICAL_FEATURES, distinct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputeValues(df, dftest):\n",
    "    categoricalColumnstoImpute = CATEGORICAL_FEATURES[1:]\n",
    "    \n",
    "    # Impute categorical features\n",
    "    for col in categoricalColumnstoImpute:\n",
    "        mostCommon = df.select(col).groupby(col).count()\\\n",
    "                            .orderBy('count', ascending=False) \\\n",
    "                            .limit(1).collect()[0][0]\n",
    "        if mostCommon == \"\":\n",
    "            mostCommon = \"EMPTY\"\n",
    "        \n",
    "        print(f\"Column {col} has most common {mostCommon}\")\n",
    "        \n",
    "        df = df.withColumn(col, F.when((df[col].isNull() | (df[col] == '')), mostCommon) \\\n",
    "                                .otherwise(df[col]))\n",
    "        \n",
    "        dftest = dftest.withColumn(col, F.when((dftest[col].isNull() | (dftest[col] == '') | (~dftest[col].isin(distinctDict[col]))), mostCommon) \\\n",
    "                        .otherwise(dftest[col]))\n",
    "    print(\"Successfully Imputed Categorical Values\")\n",
    "    \n",
    "    # Assure there is no missing values\n",
    "    for col in categoricalColumnstoImpute:\n",
    "        assert df.filter(df[col].isNull()).count() == 0, f\"Column {col} contains NULL value(s)\"\n",
    "        assert df.filter(df[col] == '').count() == 0, f\"Column {col} contains empty string(s)\"\n",
    "    \n",
    "        assert dftest.filter(dftest[col].isNull()).count() == 0, f\"Column {col} contains NULL value(s)\"\n",
    "        assert dftest.filter(dftest[col] == '').count() == 0, f\"Column {col} contains empty string(s)\"\n",
    "    \n",
    "    print(\"Successfully Imputed All Values and Passed Tests\")\n",
    "    return (df, dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "trainDF, testDF = imputeValues(trainDF, testDF)\n",
    "print(f\"\\n... Executed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "trainDF.write.parquet('trainImputed.parquet')\n",
    "testDF.write.parquet('testImputed.parquet')\n",
    "print(f\"\\n... Executed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here once imputed files are written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "trainDF = sqlContext.read.parquet('trainImputed.parquet')\n",
    "testDF = sqlContext.read.parquet('testImputed.parquet')\n",
    "# print(f\"\\n... Executed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = trainDF.columns\n",
    "# INTEGER_FEATURES = HEADER[0:14] # These are the integer features\n",
    "CATEGORICAL_FEATURES = HEADER[1:] # These are the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTEGER_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATEGORICAL_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresToDrop = set()\n",
    "featuresToDrop.add('x11')\n",
    "featuresToDrop.add('x13')\n",
    "featuresToDrop.add('x14')\n",
    "featuresToDrop.add('x18')\n",
    "featuresToDrop.add('x21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featuresToDrop = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "\n",
    "# assembler = VectorAssembler(inputCols = INTEGER_FEATURES[1:], outputCol = 'integerFeatures')\n",
    "# trainDF = assembler.transform(trainDF)\n",
    "# testDF = assembler.transform(testDF)\n",
    "\n",
    "# scaler = StandardScaler(inputCol=\"integerFeatures\", outputCol=\"scaledFeatures\",\n",
    "#                         withStd=True, withMean=False)\n",
    "\n",
    "# scalerModel = scaler.fit(trainDF)\n",
    "\n",
    "# trainDF = scalerModel.transform(trainDF)\n",
    "# testDF = scalerModel.transform(testDF)\n",
    "# print(f\"\\n... Executed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featuresToDrop = featuresToDrop.union(set(INTEGER_FEATURES[1:]))\n",
    "# featuresToDrop.add('integerFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop the Integer Columns\n",
    "trainDF = trainDF.select([c for c in trainDF.columns if c not in featuresToDrop])\n",
    "testDF = testDF.select([c for c in testDF.columns if c not in featuresToDrop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE_CATEGORICAL_FEATURES = set(CATEGORICAL_FEATURES) #-featuresToDrop # Remaining Categorical Features for the One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code help from https://www.youtube.com/watch?v=CdHuLGuU2c4\n",
    "\n",
    "cols_in = ['x1','x2', 'x3','x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x12',\n",
    "        'x15', 'x16', 'x17', 'x19', 'x20', 'x22', 'x23', 'x24', 'x25', 'x26', 'x27',\n",
    "        'x28', 'x29', 'x30', 'x31', 'x32', 'x33', 'x34', 'x35', 'x36', 'x37', 'x38', 'x39']\n",
    "\n",
    "cols_out = ['x1_OHE','x2_OHE', 'x3_OHE','x4_OHE', 'x5_OHE', 'x6_OHE', 'x7_OHE', 'x8_OHE', 'x9_OHE', 'x10_OHE', 'x12_OHE',\n",
    "        'x15_OHE', 'x16_OHE', 'x17_OHE', 'x19_OHE', 'x20_OHE', 'x22_OHE', 'x23_OHE', 'x24_OHE', 'x25_OHE', 'x26_OHE', 'x27_OHE',\n",
    "        'x28_OHE', 'x29_OHE', 'x30_OHE', 'x31_OHE', 'x32_OHE', 'x33_OHE', 'x34_OHE', 'x35_OHE', 'x36_OHE', 'x37_OHE', 'x38_OHE', 'x39_OHE']\n",
    "\n",
    "# String Indexing for categorical features\n",
    "indexers = [StringIndexer(inputCol=x, outputCol=x+\"_tmp\") for x in cols_in]\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "encoders = [OneHotEncoder(dropLast=False, inputCol=x+\"_tmp\", outputCol=y) for x,y in zip(cols_in, cols_out)]\n",
    "    \n",
    "tmp = [[i,j] for i,j in zip(indexers, encoders)]\n",
    "tmp = [i for sublist in tmp for i in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = cols_out, outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"y\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp += [assembler,labelIndexer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData = pipeline.fit(trainDF).transform(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alltestData = pipeline.fit(testDF).transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[y: float, x1: string, x2: string, x3: string, x4: string, x5: string, x6: string, x7: string, x8: string, x9: string, x10: string, x12: string, x15: string, x16: string, x17: string, x19: string, x20: string, x22: string, x23: string, x24: string, x25: string, x26: string, x27: string, x28: string, x29: string, x30: string, x31: string, x32: string, x33: string, x34: string, x35: string, x36: string, x37: string, x38: string, x39: string, x1_tmp: double, x1_OHE: vector, x2_tmp: double, x2_OHE: vector, x3_tmp: double, x3_OHE: vector, x4_tmp: double, x4_OHE: vector, x5_tmp: double, x5_OHE: vector, x6_tmp: double, x6_OHE: vector, x7_tmp: double, x7_OHE: vector, x8_tmp: double, x8_OHE: vector, x9_tmp: double, x9_OHE: vector, x10_tmp: double, x10_OHE: vector, x12_tmp: double, x12_OHE: vector, x15_tmp: double, x15_OHE: vector, x16_tmp: double, x16_OHE: vector, x17_tmp: double, x17_OHE: vector, x19_tmp: double, x19_OHE: vector, x20_tmp: double, x20_OHE: vector, x22_tmp: double, x22_OHE: vector, x23_tmp: double, x23_OHE: vector, x24_tmp: double, x24_OHE: vector, x25_tmp: double, x25_OHE: vector, x26_tmp: double, x26_OHE: vector, x27_tmp: double, x27_OHE: vector, x28_tmp: double, x28_OHE: vector, x29_tmp: double, x29_OHE: vector, x30_tmp: double, x30_OHE: vector, x31_tmp: double, x31_OHE: vector, x32_tmp: double, x32_OHE: vector, x33_tmp: double, x33_OHE: vector, x34_tmp: double, x34_OHE: vector, x35_tmp: double, x35_OHE: vector, x36_tmp: double, x36_OHE: vector, x37_tmp: double, x37_OHE: vector, x38_tmp: double, x38_OHE: vector, x39_tmp: double, x39_OHE: vector, features: vector, label: double]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, validData = allData.randomSplit([0.8,0.2], seed=1)\n",
    "# # print(\"Distribution of Positive and Negative in trainData is: \", trainData.groupBy(\"label\").count().take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainData, validData = allData.randomSplit([0.8,0.2], seed=1)\n",
    "# # print(\"Distribution of Positive and Negative in trainData is: \", trainData.groupBy(\"label\").count().take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "randforest = RF(labelCol=\"label\", featuresCol=\"features\", numTrees = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fit = randforest.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = rf_fit.transform(validData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chk Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = transformed.select([\"probability\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_collect = results.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = [(float(i[0][0]),1.0-float(i[1])) for i in results_collect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = sc.parallelize(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = metric(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The ROC score is (numTrees=100): \", metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample to test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF = trainDF.sample(False, 0.0001, seed=1234) #.toPandas() # Approximately 4500 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleData = pipeline.fit(sampleDF).transform(sampleDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, validData = sampleData.randomSplit([0.8,0.2], seed=1)\n",
    "# print(\"Distribution of Positive and Negative in trainData is: \", trainData.groupBy(\"label\").count().take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "randforest = RF(labelCol=\"label\", featuresCol=\"features\", numTrees = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fit = randforest.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = rf_fit.transform(validData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chk Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = transformed.select([\"probability\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_collect = results.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = [(float(i[0][0]),1.0-float(i[1])) for i in results_collect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = sc.parallelize(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = metric(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC score is (numTrees=100):  0.6816261914088002\n"
     ]
    }
   ],
   "source": [
    "print(\"The ROC score is (numTrees=100): \", metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
